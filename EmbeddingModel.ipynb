{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e2bf86",
   "metadata": {},
   "source": [
    "## Embedding model ##\n",
    ">Floris Menninga \\\n",
    ">Datum: 8-01-2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be72d87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 20:20:58.171207: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769714458.225788    4971 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769714458.241677    4971 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769714458.350537    4971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769714458.350557    4971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769714458.350560    4971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769714458.350562    4971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-29 20:20:58.364859: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# %pip install tensorflow keras\n",
    "# #%pip install tqdm\n",
    "# %pip install tensorflow_text\n",
    "# %pip install bs4\n",
    "# %pip install joblib\n",
    "# %pip install lxml\n",
    "\n",
    "from data_parser import xml_parser\n",
    "import EmbeddingModel\n",
    "\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import lxml\n",
    "\n",
    "%load_ext tensorboard\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "SEED = 0\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea16484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gpu's:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 20:21:00.696013: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2026-01-29 20:21:00.696162: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
      "2026-01-29 20:21:00.696173: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: FLORIS-2\n",
      "2026-01-29 20:21:00.696182: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: FLORIS-2\n",
      "2026-01-29 20:21:00.696237: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program. The library may be missing or provided via another object.\n",
      "2026-01-29 20:21:00.696392: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 580.119.2\n"
     ]
    }
   ],
   "source": [
    "os.environ['LD_LIBRARY_PATH'] = '/run/opengl-driver/lib:' + os.environ.get('LD_LIBRARY_PATH', '')\n",
    "print(\"Gpu's: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f3220",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "De eerste stap is het omzetten van de text in tokens:\n",
    "In tegenstelling to methodes zoals bytepair encoding etc. worden de tokens gemaakt door de zinnen \\\n",
    "te splitten op spaties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d82f9469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hallo, dit is een testzin om het programma te testen.\"\n",
    "tokens = list(sentence.lower().split())\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0999d3e5",
   "metadata": {},
   "source": [
    "### Vocabulary maken van deze tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6de822cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'hallo,': 1, 'dit': 2, 'is': 3, 'een': 4, 'testzin': 5, 'om': 6, 'het': 7, 'programma': 8, 'te': 9, 'testen.': 10}\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1 \n",
    "vocab['<pad>'] = 0\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf9e31",
   "metadata": {},
   "source": [
    "### Inverse vocabulary:\n",
    "Van integer index naar token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd28b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: 'hallo,', 2: 'dit', 3: 'is', 4: 'een', 5: 'testzin', 6: 'om', 7: 'het', 8: 'programma', 9: 'te', 10: 'testen.'}\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f79c57",
   "metadata": {},
   "source": [
    "## Vectorizeren van de zin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e953496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "test_sequence = [vocab[word] for word in tokens]\n",
    "\n",
    "print(test_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71d341",
   "metadata": {},
   "source": [
    "## Skip-gram maken:\n",
    "Het embedding model zal gemaakt worden met behulp van het Word2Vec algorithme. Verdere uitleg over hoe dit algorithme werkt komt straks maar om dit te kunnen gebruiken \n",
    "moet er eerst een Skip-gram model gemaatk worden. \n",
    "Het belangrijkste punt van een skip-gram is dat de vector representatie gebaseerd is op de ruimtelijke nabijheid van woorden in een zin. \n",
    "Als het woord \"Ketting\" altijd gevolgd zou worden door het woord \"zaag\" zal deze combinatie van woorden vaker voorkomen dan in andere combinaties.\n",
    "Een skip-gram model is een klein neural network met een inputlayer, embedding layer en output layer. \n",
    "Dit model moet een waarschijnlijkheids verdelings vector geven voor een gegeven input woord. Dat is dus de kans dat deze twee woorden samen voorkomen binnen de context lengte waarmee het model getrained is. De som van deze waarschijnlijkheids verdeling is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9659dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      test_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0,\n",
    "      seed=0\n",
    ")\n",
    "\n",
    "print(len(positive_skip_grams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7f91b",
   "metadata": {},
   "source": [
    "Hieronder staan vijf van de gegenereerde skip-grams. \n",
    "Zoals in de bovenstaande code gedefinieerde \"window size\" bestaat elk skip-gram uit twee tokens. \n",
    "In dit geval zijn het hele woorden omdat het zo getokenizeerd is. \n",
    "Later wil ik nog proberen om andere tokenizatie technieken te gebruiken zoals byte pair encoding of wordpiece / sentencepiece.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf2e2e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4): (is, een)\n",
      "(1, 2): (hallo,, dit)\n",
      "(8, 9): (programma, te)\n",
      "(6, 8): (om, programma)\n",
      "(4, 3): (een, is)\n"
     ]
    }
   ],
   "source": [
    "for target, context in positive_skip_grams[:5]:\n",
    "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a007e49d",
   "metadata": {},
   "source": [
    "### Negative sampling\n",
    "\n",
    "De spikgrams functie retourneerd alle positieve skip-gram paren door met een sliding window van \n",
    "een gegeven grootte over de tekst te gaan. Voor training is dit echter niet genoeg, er moeten ook negatieve samples bij zitten. Deze negatieve skip-gram paren worden verkregen door willekeurige woorden uit de vocabulary te halen en deze samen te voegen. \n",
    "\n",
    "Nu zal er gebruik gemaakt worden van de functie \"tf.random.log_uniform_candidate_sampler\"\n",
    "om een aantal (num_ns) negatieve samples voor een gegeven target woord in een window te krijgen.\n",
    "Voor het trainen kan het."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3984ead8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([7 1 6 0], shape=(4,), dtype=int64)\n",
      "['het', 'hallo,', 'om', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,\n",
    "    num_true=1,\n",
    "    num_sampled=num_ns,\n",
    "    unique=True,\n",
    "    range_max=vocab_size,\n",
    "    name=\"negative_sampling\"\n",
    ")\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbf3fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezed_context_class = tf.squeeze(context_class, 1)\n",
    "\n",
    "context = tf.concat([squeezed_context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "target = target_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ae17fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 3\n",
      "target_word     : is\n",
      "context_indices : [4 7 1 6 0]\n",
      "context_words   : ['een', 'het', 'hallo,', 'om', '<pad>']\n",
      "label           : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41e89f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
      " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
    "print(sampling_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5254fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sequences, window_size, num_ns, vocab_size):\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=10)\n",
    "\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29825b60",
   "metadata": {},
   "source": [
    "## Word2Vec:\n",
    "\n",
    "Het word2vec algorithme is een word embedding techniek in natural language processing die er voor zorgt dat woorden als vectors gerepresenteerd kunnen worden in een continue vector ruimte. \n",
    "Dit kan vervolgens gebruikt worden om relaties tussen woorden te achterhalen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20fa49b",
   "metadata": {},
   "source": [
    "Hier zal de dataset ingeladen worden:\n",
    "Lees dataset regel voor regel en verwijder lege regels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b30b4ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_ds = tf.data.TextLineDataset([\"/run/media/floris/FLORIS_3/Data_set/PubMed/dataset_100914.txt\"]).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "text_ds = tf.data.TextLineDataset([\"/home/floris/Documenten/Dataset/trainingsData.txt\"]).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "\n",
    "# Verwijder punctuatie / hoofdletters.\n",
    "def standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64210d",
   "metadata": {},
   "source": [
    "Leer de vocabulary van de data, de top 20000 meest frequente woorden krijgen een nummer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cbb9412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aantal woorden in seq en vocabulary grootte:\n",
    "sequence_length = 200\n",
    "vocab_size = 20000\n",
    "\n",
    "# Vectorize the layer en split en map strings tokens met TextVectorization:\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74248f16",
   "metadata": {},
   "source": [
    "Batch de data voor snellere verwerking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60dc5427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 20:21:01.789942: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee5c7d",
   "metadata": {},
   "source": [
    "Gebruik de originele traininstext en gebruik de \"vectorize_layer\" met integers voor alle 20000 meest voorkomende tokens. (in dit geval woorden) Nu is er dus een array met integers die deze woorden representeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f629d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectoriseer de data in text_ds:\n",
    "text_vector_ds = text_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73b2e9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 20:21:02.673017: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Maak een lijst van deze gevectoriseerde data:\n",
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb83156",
   "metadata": {},
   "source": [
    "De functie generate_training_data() maakt eerst een skip-gram, het kiest eerst een target woord\n",
    "en kijkt binnen het gekozen window, bijvoorbeeld 5 woorden.\n",
    "Als het andere woord binnen de window voorkomt kan een positief paar gevormd worden. \n",
    "dit zijn de integers waaruit de vocabulary bestaat, bijvoorbeeld (4,3).\n",
    "\n",
    "Als het model alleen positieve voorbeelden zou krijgen zou het niet kunnen leren omdat het alles als positief zou classificeren. Om dit tegen te gaan worden ook negatieve paren gegenereerd. Dit zijn woorden die niet binnen het context window samen voorkomen. \n",
    "Deze woorden worden willekeurig gekozen.\n",
    "\n",
    "Later in de functie worden de ware paren en de negatieve paren gecombineerd in een lijst, de context lijst en de even lange \"labels\" lijst die voor elke waarde in de context lijst een 0 of een 1 bevat, 0 voor een negatief paar en 1 voor een positief paar. \n",
    "\n",
    "Alle target woorden komen meerdere keren voor in de targets lijst zodat het model er goed van kan leren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abb1db7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6933/6933 [37:44<00:00,  3.06it/s]  \n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=5,\n",
    "    num_ns=5,\n",
    "    vocab_size=vocab_size)\n",
    "\n",
    "# print(f\"targets.shape: {targets.shape}\")\n",
    "# print(f\"contexts.shape: {contexts.shape}\")\n",
    "# print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace14268",
   "metadata": {},
   "source": [
    "Gegenereerde data opslaan:\n",
    "Het duurde lang om te genereren dus sla ik het op in een .joblib bestand, dit zal ik ook voor sommige vervolg stappen doen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ae09ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib_list = [targets, contexts, labels]\n",
    "\n",
    "# joblib.dump(joblib_list, \"targets_contexts_labels.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdbede8",
   "metadata": {},
   "source": [
    "Maak de dataset die gebruikt zal worden voor het trainen:\n",
    "Eerst wordt er een tuple gemaakt van het target woord, de context en de labels.\n",
    "Voor het trainen zal de X dus de tuple zijn en de y de labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df2aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3960ce",
   "metadata": {},
   "source": [
    "Het Word2Vec model dat gebruikt zal worden voor het trainen bestaat uit twee\n",
    "embedding layers met als input layer (ter grootte van het aantal samples per batch).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e7574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Word2Vec(tf.keras.Model):\n",
    "#   def __init__(self, vocab_size, embedding_dim):\n",
    "#     super(Word2Vec, self).__init__()\n",
    "#     self.target_embedding = layers.Embedding(vocab_size,embedding_dim,name=\"w2v_embedding\")\n",
    "#     self.context_embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "#   def call(self, pair):\n",
    "#     target, context = pair\n",
    "#     # context: (batch, context)\n",
    "#     if len(target.shape) == 2:\n",
    "#       target = tf.squeeze(target, axis=1)\n",
    "#     # target: (batch,)\n",
    "#     word_emb = self.target_embedding(target)\n",
    "#     # word_emb: (batch, embed)\n",
    "#     context_emb = self.context_embedding(context)\n",
    "#     # context_emb: (batch, context, embed)\n",
    "#     dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "#     # dots: (batch, context)\n",
    "#     return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee88b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,embedding_dim,name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "\n",
    "        word_emb = self.target_embedding(target)\n",
    "        context_emb = self.context_embedding(context)\n",
    "\n",
    "        # Normaliseer vectoren naar lengte 1:\n",
    "        word_emb = tf.nn.l2_normalize(word_emb, axis=-1)\n",
    "        context_emb = tf.nn.l2_normalize(context_emb, axis=-1)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        return dots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcb5327",
   "metadata": {},
   "source": [
    "## Verschil tussen maten van gelijkenis tussen vectoren\n",
    "\n",
    "De bovenstaande functie gebruikt het dot product om gelijkheid tussen de vectoren te berekenen.\n",
    "Dit is niet de enige methode die gebruikt kan worden. Een andere die veel gebruikt is bij machine learning is cosine similarity, ook wordt de euclidische afstand wel gebruikt. \n",
    "\n",
    "Het dot product, dat gebruikt wordt in het bovenstaande codeblok (afkomstig van een tensorflow voorbeeld op https://www.tensorflow.org/) vermenigvuldiged de nummers in de vectors en sommeerd ze.\n",
    "Het is een maat die de richting van de vector en de magnitude bevat. Er onder staat een test waar ik het dot product vervangen heb door cosine similarity.\n",
    "Dit is ook sneller om door de computer uit te laten rekenen dat bijvoorbeeld cosine similarity.\n",
    "Bij embedding modellen hebben woorden die frequent voorkomen een langere vector (grotere magnitude)\n",
    "\n",
    "Cosine similarity is vergelijkbaar met het dot product maar dan gedeeld door de lengte van de vectors. De lengte van alle vectoren zal dus 1 zijn en alleen de richting van de vectoren is een factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6975db19",
   "metadata": {},
   "source": [
    "Hier wordt het Word2Vec algorithme gebruikt\n",
    "Hier is de [Adam](https://arxiv.org/abs/1412.6980) optimizer gebruikt, dit is een algorithme dat de learning rate aan kan passen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04301a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 48\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam', \n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15cd1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Om tensorboard te kunnen gebruiken om de trainingsvoortgang te kunnen zien:\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db8e071",
   "metadata": {},
   "source": [
    "### Train model:\n",
    "\n",
    "Het model zal nu getrained worden met als loss functie categoricalcrossentropy om de prestaties van het model te scoren. Het model krijgt veel meer negatieve samples dan positieve dus moet het zeker zijn in de keuzes. Het embedden van tekst is dus een classificatie probleem geworden.\n",
    "De softmax functie veranderd de scores in percentages met een totaal van 100%. (De kans dat het label correct is)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e85f6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 7ms/step - accuracy: 0.5895 - loss: 1.0613\n",
      "Epoch 2/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 7ms/step - accuracy: 0.5973 - loss: 1.0306\n",
      "Epoch 3/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 7ms/step - accuracy: 0.6085 - loss: 1.0099\n",
      "Epoch 4/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 7ms/step - accuracy: 0.6189 - loss: 0.9922\n",
      "Epoch 5/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 7ms/step - accuracy: 0.6255 - loss: 0.9817\n",
      "Epoch 6/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 7ms/step - accuracy: 0.6299 - loss: 0.9751\n",
      "Epoch 7/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 7ms/step - accuracy: 0.6331 - loss: 0.9705\n",
      "Epoch 8/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 7ms/step - accuracy: 0.6355 - loss: 0.9671\n",
      "Epoch 9/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 7ms/step - accuracy: 0.6374 - loss: 0.9644\n",
      "Epoch 10/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 7ms/step - accuracy: 0.6388 - loss: 0.9623\n",
      "Epoch 11/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6401 - loss: 0.9606\n",
      "Epoch 12/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 7ms/step - accuracy: 0.6412 - loss: 0.9591\n",
      "Epoch 13/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 7ms/step - accuracy: 0.6422 - loss: 0.9578\n",
      "Epoch 14/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 7ms/step - accuracy: 0.6430 - loss: 0.9568\n",
      "Epoch 15/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 7ms/step - accuracy: 0.6438 - loss: 0.9558\n",
      "Epoch 16/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 7ms/step - accuracy: 0.6444 - loss: 0.9549\n",
      "Epoch 17/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6450 - loss: 0.9542\n",
      "Epoch 18/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 7ms/step - accuracy: 0.6455 - loss: 0.9535\n",
      "Epoch 19/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6460 - loss: 0.9529\n",
      "Epoch 20/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6464 - loss: 0.9523\n",
      "Epoch 21/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 7ms/step - accuracy: 0.6468 - loss: 0.9518\n",
      "Epoch 22/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6472 - loss: 0.9513\n",
      "Epoch 23/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 7ms/step - accuracy: 0.6475 - loss: 0.9508\n",
      "Epoch 24/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 7ms/step - accuracy: 0.6479 - loss: 0.9504\n",
      "Epoch 25/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6481 - loss: 0.9500\n",
      "Epoch 26/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 7ms/step - accuracy: 0.6484 - loss: 0.9496\n",
      "Epoch 27/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 7ms/step - accuracy: 0.6487 - loss: 0.9493\n",
      "Epoch 28/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 7ms/step - accuracy: 0.6489 - loss: 0.9489\n",
      "Epoch 29/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6492 - loss: 0.9486\n",
      "Epoch 30/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 7ms/step - accuracy: 0.6494 - loss: 0.9483\n",
      "Epoch 31/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 7ms/step - accuracy: 0.6496 - loss: 0.9480\n",
      "Epoch 32/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6498 - loss: 0.9477\n",
      "Epoch 33/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 7ms/step - accuracy: 0.6500 - loss: 0.9475\n",
      "Epoch 34/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6501 - loss: 0.9472\n",
      "Epoch 35/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 7ms/step - accuracy: 0.6503 - loss: 0.9470\n",
      "Epoch 36/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6505 - loss: 0.9467\n",
      "Epoch 37/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6506 - loss: 0.9465\n",
      "Epoch 38/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6508 - loss: 0.9463\n",
      "Epoch 39/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6509 - loss: 0.9461\n",
      "Epoch 40/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6510 - loss: 0.9459\n",
      "Epoch 41/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6511 - loss: 0.9457\n",
      "Epoch 42/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 7ms/step - accuracy: 0.6512 - loss: 0.9455\n",
      "Epoch 43/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 7ms/step - accuracy: 0.6513 - loss: 0.9453\n",
      "Epoch 44/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 7ms/step - accuracy: 0.6514 - loss: 0.9451\n",
      "Epoch 45/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 7ms/step - accuracy: 0.6515 - loss: 0.9449\n",
      "Epoch 46/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6516 - loss: 0.9448\n",
      "Epoch 47/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6517 - loss: 0.9446\n",
      "Epoch 48/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 7ms/step - accuracy: 0.6518 - loss: 0.9444\n",
      "Epoch 49/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7ms/step - accuracy: 0.6519 - loss: 0.9443\n",
      "Epoch 50/50\n",
      "\u001b[1m49020/49020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 7ms/step - accuracy: 0.6519 - loss: 0.9441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fb2e3549e10>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=50, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93d186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weights_vocab.joblib']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib_list = [weights, vocab]\n",
    "\n",
    "joblib.dump(joblib_list, \"weights_vocab.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7df0d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19999 vectors en metadata entries.\n"
     ]
    }
   ],
   "source": [
    "num_tokens = min(len(vocab), len(weights))\n",
    "\n",
    "with io.open('vectors.tsv', 'w', encoding='utf-8') as out_v, \\\n",
    "     io.open('metadata.tsv', 'w', encoding='utf-8') as out_m:\n",
    "\n",
    "    # Skip token 0\n",
    "    for index in range(1, num_tokens):\n",
    "        word = vocab[index]\n",
    "        vec = weights[index]\n",
    "        \n",
    "        clean_word = word.strip()\n",
    "        \n",
    "        if not clean_word:\n",
    "            clean_word = \"[UNK_EMPTY]\"\n",
    "\n",
    "        # Vector\n",
    "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "        \n",
    "        # Metadata\n",
    "        out_m.write(clean_word + \"\\n\")\n",
    "\n",
    "print(f\"{num_tokens - 1} vectors en metadata entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7320b0e5",
   "metadata": {},
   "source": [
    "## Trainen van het model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b8b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = EmbeddingModel.EmbeddingModel()\n",
    "# model.load_dataset()\n",
    "# model.create_vocab()\n",
    "# model.vectorize()\n",
    "# model.generate_trainingdata()\n",
    "# model.word2vec()\n",
    "# model.fit()\n",
    "# model.save2file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf7b532",
   "metadata": {},
   "source": [
    "## Data:\n",
    "Als trainingsdata gebruik ik 869597 Pubmed artikelen (~70GB) die ik gedownload heb van https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_bulk/oa_comm/xml/\n",
    "\n",
    "oa_comm_xml.PMC000xxxxxx.baseline.2025-06-26.tar.gz t/m oa_comm_xml.PMC004xxxxxx.baseline.2025-06-26.tar.gz\n",
    "\n",
    "Deze zijn nieuwer dan de artikelen uit 2021 die op /commons/data/NCBI/PubMed/ staan.\n",
    "\n",
    "Vervolgens moeten deze artikelen doorzocht worden op inhoud. \\\n",
    "De .xml bestanden hebben headers voor titel, abstract, body etc. deze doorzoek ik met behulp van mijn klasse: data_parser.py\n",
    "\n",
    "Het filteren op de drie keywords \"cancer\", \"melanoma\" en \"carcinoma\" duurde 9 uur en 30 minuten en reduceerde de hoeveelheid artikelen \n",
    "van 869597 naar 100914 (2.7GB).\n",
    "\n",
    "100914 Artikelen met deze zoekcriteria opgeslagen in: /run/media/floris/FLORIS_3/Data_set/PubMed/dataset_large.txt\n",
    "\n",
    "Voor lateren trainingsruns heb ik de data_parser.py aangepast zodat hij inplaats van het hele artikel op een enkele regel zet, een alinea per regel zet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1300d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3028 XML bestanden...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3028/3028 [00:50<00:00, 59.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voltooid...\n",
      "194 Artikelen met deze zoekcriteria opgeslagen in: /home/floris/Documenten/Dataset/trainingsData.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "keyword_list = [\"cancer\", \"melanoma\", \"carcinoma\"] # Niet hoofdlettergevoelig...\n",
    "\n",
    "trainings_data = xml_parser(\"/home/floris/Documenten/Dataset/PMC000xxxxxx\", \"/home/floris/Documenten/Dataset/trainingsData.txt\", keyword_list)\n",
    "\n",
    "# Filter de data:\n",
    "trainings_data.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d8136",
   "metadata": {},
   "source": [
    "Hierboven staat de uitkomst van de filterstap, van de kleinere subset van de trainingsdata zijn 194 artikelen gevonden die een (of meer) van de drie keywords bevatten in het abstract. \n",
    "Dit kwam uit op een .txt bestand van ~4.1MB maar door het genereren van de vele positieve en negatieve skip-gram combinaties vulde dit tijdens het trainen vaak het RAM geheugen van mijn laptop volledig. (64GB)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d70ea50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba0f664b",
   "metadata": {},
   "source": [
    "## Resultaten:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d8b961",
   "metadata": {},
   "source": [
    "Het eerste model was met de volgende hyperparameters getrained:\n",
    "\n",
    "vocabulary grootte: 20000 \\\n",
    "context lengte: 100 \\\n",
    "window_size=5 \\\n",
    "num_ns=4 \\\n",
    "dims = 128\n",
    "\n",
    "Het model was erg overfit op de trainingsdata, de loss was bijna 0 en accuratesse bijna 1. \\\n",
    "De volgende poging zal een minder grote vocabulary krijgen en minder dimenties om beter te generaliseren i.p.v. de trainingsdata te onthouden. \n",
    "\n",
    "De onderstaande afbeelding weergeeft de loss en accuracy grafieken met Tensorboard:\n",
    "\n",
    "![](img/run_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde0f6c",
   "metadata": {},
   "source": [
    "Zoals te verwachten is wordt de loss steeds lager."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c775c3",
   "metadata": {},
   "source": [
    "### Tweede run:\n",
    "\n",
    "Na het proberen van vele combinaties van hyperparameters lijken deze een veel beter resultaat te geven vergeleken met de vorige:\n",
    "\n",
    "vocabulary grootte: 1000 \\\n",
    "context lengte: 1000 \\\n",
    "window_size=5 \\\n",
    "num_ns=4 \\\n",
    "dims = 64\n",
    "\n",
    "![](img/run_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd4a5e",
   "metadata": {},
   "source": [
    "Training accuratesse lijkt te convergeren rondt 0.62, voor meer epochs trainen zou weinig zin hebben. Het zelfde lijkt voor de loss te gebeuren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b5341",
   "metadata": {},
   "source": [
    "### Derde run:\n",
    "\n",
    "\n",
    "vocabulary grootte: 20000 \\\n",
    "context lengte: 300 \\\n",
    "window_size=5 \\\n",
    "num_ns=5 \\\n",
    "dims = 48\n",
    "\n",
    "\n",
    "![run_3.png](img/run_3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4580647",
   "metadata": {},
   "source": [
    "De derde traininsronde presteerd het beste na de resulterende vectors.tsv en metadata.tsv in de projector.tensorflow.org website te visualiseren en de gelijkenis tussen een aantal woorden op te zoeken. \n",
    "De voorgaande modelen lijken veel meer willekeurig lijkende woorden in de lijst te hebben dan dit model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d31974",
   "metadata": {},
   "source": [
    "Wanneer we naar de nabijgelegen woorden in de vectorspace kijken zijn de woorden die er het dichtst bij liggen \"ductal\", \"frequency\" en \"invasion\". \n",
    "Een \"ductal carcinoma\" is een vroege form van borstkanker en aangezien het woord ductal waarschijnlijk niet al te vaak voorkomt in andere context (in deze artikelen) zal het vrijwel altijd met kanker geassocieerd worden.\n",
    "Het woord ductal kwam 36 keer voor in de trainingsdata.\n",
    "\n",
    "Het woord \"malignency\" heeft het woord \"cancers\" maar ook \"tyrosine\" , misschien komt dit woord er voor omdat tyrosine kinase inhibitors ook tegen kanker gebruikt kunnen worden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0098e",
   "metadata": {},
   "source": [
    "Deze laatste trainingsronde heb ik met een kleinere subset van de artikelen gemaakt dan de eerste. Het gefilterde .txt bestand bevatte maar 197 artikelen.\n",
    "\n",
    "Het aantal regels: `wc -l trainingData.txt` = 6934 regels \\\n",
    "Aantal woorden `wc -w trainingData.txt` = 607965 woorden \\\n",
    "\n",
    "Voor de keywords waarop ik gefilterd heb, \"cancer\", \"melanoma\" en \"carcinoma\", dit is hoeveel zinnen deze woorden bevatten: \\\n",
    "`grep -o \"cancer\" trainingsData.txt | wc -l` = 4511 \\\n",
    "`grep -o \"melanoma\" trainingsData.txt | wc -l` = 588 \\\n",
    "`grep -o \"carcinoma\" trainingsData.txt | wc -l` = 1137 \\\n",
    " \n",
    "\n",
    "### Frequentie analyse trainingstekst:\n",
    "\n",
    "`cat trainingsData.txt | tr -s ' ' '\\n' | sort | uniq -c | sort -nr | head -50` =\n",
    "\n",
    "```\n",
    "  27802 the\n",
    "  24475 of\n",
    "  17275 and\n",
    "  14315 in\n",
    "  11189 to\n",
    "   8467 a\n",
    "   6657 with\n",
    "   6243 for\n",
    "   5475 were\n",
    "   5346 was\n",
    "   4782 that\n",
    "   4563 is\n",
    "   4321 The\n",
    "   3910 by\n",
    "   3545 as\n",
    "   3086 cancer\n",
    "   2950 or\n",
    "   2854 be\n",
    "   2826 from\n",
    "   2671 are\n",
    "   2635 cells\n",
    "   2347 on\n",
    "   2341 at\n",
    "   2235 patients\n",
    "   2149 not\n",
    "   2129 cell\n",
    "   2056 tumor\n",
    "   1880 this\n",
    "   1826 have\n",
    "   1703 In\n",
    "   1675 an\n",
    "   1482 expression\n",
    "   1365 which\n",
    "   1322 been\n",
    "   1258 these\n",
    "   1203 between\n",
    "   1192 we\n",
    "   1189 may\n",
    "   1175 has\n",
    "   1112 using\n",
    "   1111 than\n",
    "   1105 study\n",
    "   1103 also\n",
    "   1094 data\n",
    "    986 breast\n",
    "    979 used\n",
    "    918 This\n",
    "    915 more\n",
    "    910 all\n",
    "    901 can\n",
    "\n",
    "```\n",
    "\n",
    "Dit bovenstaande bash statement wordt elk woord in een nieuwe regel gezet, het blijkt dus dat de \"stopwoorden\" die in normale zinnen voorkomen. Dit is voor de 50 meest frequent voorkomende woorden gedaan. \n",
    "Ik heb uit verschillende bronnen tegensprekende informatie gevonden ([Verwijderen van stopwoorden](\"https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\") en [Niet verwijderen van stopwoorden](\"https://maartengr.github.io/BERTopic/getting_started/tips_and_tricks/tips_and_tricks.html\")) over het nut van het verwijderen van stopwoorden. \\\n",
    "Uiteindelijk heb ik er voor gekozen om ze niet te verwijderen omdat deze woorden toch waarschijnlijk een belangrijke rol spelen in de relaties tussen woorden in zinnen. \\\n",
    "Ook het keyword \"cancer\" waar ik op gefilderd heb komt voor in de top 50, mijn speculatie is dat het nu zo onevenredig vaak voorkomt dat het woord geassocieerd wordt met te veel andere woorden. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e2de82",
   "metadata": {},
   "source": [
    "# Conclusie / Reflectie:\n",
    "\n",
    "Het Vec2Word embedding model dat resulteerde van de trainingscode in dit notebook presteerd minder goed dan gehoopt. Het derde model was het beste met de laagste loss en hoogste acuratesse. Dit model was getrained voor 50 epochs tegenover de 20 van de eerste. De tweede had ook voor 50 epochs getrained maar de slechte prestaties kunnen mogelijk toegewezen worden aan de te kleine vocabulary, het model kent niet genoeg woorden om toevallig de woorden er bij te hebben die vaak genoeg voorkwamen toe te voegen aan de vocab. (in de top 1000 van meeste frequente woorden omdat de vocabulary grootte 1000 was.)\n",
    "Na het bekijken van de woorden die het meeste overeenkomst vertonen met elkaar, volgens het model zaten veel termen die wel verwant zijn aan welkaar\n",
    "maar er zat ook veel, op het oog onzinnige data tussen die niets te maken heeft met het gekozen target woord. \n",
    "\n",
    "Er zijn nog vele andere dingen die gedaan kunnen worden om potentieel het model beter te laten werken zoals: beter filteren, bijvoorbeeld op meer termen in plaats van de drie die nu gebruikt worden. \n",
    "Omdat het trainen lang duurde is het niet goed mogelijk om een soort grid search voor de optimale hyperparameters uit te voeren maar er zijn wel enkele combinaties die ik, als er meer tijd was, zou willen proberen zoals de context window lengte korter maken zodat het model minder goed verbanden tussen complexere concepten zou \"begrijpen\" maar dat woorden die bijna altijd samen voorkomen ook beter terug komen. Zo zou een weinig voorkomend woord zoals \"ductal\" in deze artikelen bijna altijd voorkomen in de combinatie ductal carcinoma terwijl dit nu niet zo is bij vergelijkbare woorden. Het woord ductal zelf kwam niet vaak genoeg voor om een eigen nummer te krijgen en zou dus een \"UNK\" (unknown) token krijgen. \n",
    "\n",
    "Een ander punt is dat ik een andere tokenzer had kunnen gebruiken zoals byte pair encoding waar substring die vaken voorkomen in verschillende woorden gebruikt worden als tokens zodat de vocabulary minder groot hoeft te zijn om de zelfde woorden / texten te bevatten.\n",
    "Verder speculeer ik dat het belangrijkste punt dat het model beter zou kunnen maken het gebruiken van meer trainingsdata is. Ik had in totaal ongeveer 70GB aan Pubmed artikelen gedownload maar hier heb ik uiteidelijk maar een kleine subset van gebruikt omdat de trainingstijd enorm lang is. Maar door geen gebruik te maken van batches en niet de volledige trainingsdataset in RAM geheugen te laden zou het wel mogelijk zijn om het model te trainen met de volledige dataset. Nvidia CUDA zou het trainen ook sneller moeten maken maar dat werkte niet altijd in dit notebook. Ook was het verschil in trainingssnelheid minder groot dan verwacht op een Nvidia RTX 3070.\n",
    "\n",
    "Nog een ander onderdeel waar ik meer mee had kunnen experimenteren was het gebruiken van een andere optimizer dan Adam, een andere loss functie dan CrossEntropyloss en andere vector gelijkenis functies dan het dot product."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Embedding_Model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
