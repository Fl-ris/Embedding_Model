{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e2bf86",
   "metadata": {},
   "source": [
    "## Embedding model ##\n",
    ">Floris Menninga \\\n",
    ">Datum: 8-01-2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be72d87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-19 15:57:38.636175: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768834659.193557   32357 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768834659.397676   32357 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768834659.770693   32357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768834659.770719   32357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768834659.770722   32357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768834659.770724   32357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-19 15:57:39.782400: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/floris/.local/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "# %pip install tensorflow keras\n",
    "# %pip install tqdm\n",
    "# %pip install tensorflow_text\n",
    "#%pip install bs4\n",
    "#%pip install joblib\n",
    "\n",
    "from data_parser import xml_parser\n",
    "import EmbeddingModel\n",
    "\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import lxml\n",
    "\n",
    "%load_ext tensorboard\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea16484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gpu's:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Gpu's: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f3220",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "De eerste stap is het omzetten van de text in tokens:\n",
    "In tegenstelling to methodes zoals bytepair encoding etc. worden de tokens gemaakt door de zinnen \\\n",
    "te splitten op spaties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82f9469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hallo, dit is een testzin om het programma te testen.\"\n",
    "tokens = list(sentence.lower().split())\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0999d3e5",
   "metadata": {},
   "source": [
    "### Vocabulary maken van deze tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de822cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'hallo,': 1, 'dit': 2, 'is': 3, 'een': 4, 'testzin': 5, 'om': 6, 'het': 7, 'programma': 8, 'te': 9, 'testen.': 10}\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1 \n",
    "vocab['<pad>'] = 0\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf9e31",
   "metadata": {},
   "source": [
    "### Inverse vocabulary:\n",
    "Van integer index naar token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fd28b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: 'hallo,', 2: 'dit', 3: 'is', 4: 'een', 5: 'testzin', 6: 'om', 7: 'het', 8: 'programma', 9: 'te', 10: 'testen.'}\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f79c57",
   "metadata": {},
   "source": [
    "## Vectorizeren van de zin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e953496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "test_sequence = [vocab[word] for word in tokens]\n",
    "\n",
    "print(test_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71d341",
   "metadata": {},
   "source": [
    "## Skip-gram maken:\n",
    "Het embedding model zal gemaakt worden met behulp van het Word2Vec algorithme. Verdere uitleg over hoe dit algorithme werkt komt straks maar om dit te kunnen gebruiken \n",
    "moet er eerst een Skip-gram model gemaatk worden. \n",
    "Het belangrijkste punt van een skip-gram is dat de vector representatie gebaseerd is op de ruimtelijke nabijheid van woorden in een zin. \n",
    "Als het woord \"Ketting\" altijd gevolgd zou worden door het woord \"zaag\" zal deze combinatie van woorden vaker voorkomen dan in andere combinaties.\n",
    "Een skip-gram model is een klein neural network met een inputlayer, embedding layer en output layer. \n",
    "Dit model moet een waarschijnlijkheids verdelings vector geven voor een gegeven input woord. Dat is dus de kans dat deze twee woorden samen voorkomen binnen de context lengte waarmee het model getrained is. De som van deze waarschijnlijkheids verdeling is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9659dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      test_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0,\n",
    "      seed=0\n",
    ")\n",
    "\n",
    "print(len(positive_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf2e2e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4): (is, een)\n",
      "(1, 2): (hallo,, dit)\n",
      "(8, 9): (programma, te)\n",
      "(6, 8): (om, programma)\n",
      "(4, 3): (een, is)\n"
     ]
    }
   ],
   "source": [
    "for target, context in positive_skip_grams[:5]:\n",
    "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3984ead8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 6  1  5  0  8  9  7  2 10  4], shape=(10,), dtype=int64)\n",
      "['om', 'hallo,', 'testzin', '<pad>', 'programma', 'te', 'het', 'dit', 'testen.', 'een']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1768085204.341126   91755 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4649 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "num_ns = 10\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\n",
    "    unique=True,  # all the negative samples should be unique\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
    "    seed=SEED,  # seed for reproducibility\n",
    "    name=\"negative_sampling\"  # name of this operation\n",
    ")\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbf3fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce a dimension so you can use concatenation (in the next step).\n",
    "squeezed_context_class = tf.squeeze(context_class, 1)\n",
    "\n",
    "# Concatenate a positive context word with negative sampled words.\n",
    "context = tf.concat([squeezed_context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "target = target_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ae17fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 3\n",
      "target_word     : is\n",
      "context_indices : [ 4  6  1  5  0  8  9  7  2 10  4]\n",
      "context_words   : ['een', 'om', 'hallo,', 'testzin', '<pad>', 'programma', 'te', 'het', 'dit', 'testen.', 'een']\n",
      "label           : [1 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41e89f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
      " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
    "print(sampling_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=10,\n",
    "          seed=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=0,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29825b60",
   "metadata": {},
   "source": [
    "## Word2Vec:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20fa49b",
   "metadata": {},
   "source": [
    "Hier zal de dataset ingeladen worden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30b4ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_ds = tf.data.TextLineDataset([\"/run/media/floris/FLORIS_3/Data_set/PubMed/dataset_100914.txt\"]).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "text_ds = tf.data.TextLineDataset([\"/home/floris/Downloads/trainingsData.txt\"]).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64210d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cbb9412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "sequence_length = 5\n",
    "vocab_size = 25000\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60dc5427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 23:46:51.390358: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af43d40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', np.str_('the'), np.str_('of'), np.str_('and'), np.str_('in'), np.str_('to'), np.str_('a'), np.str_('with'), np.str_('for'), np.str_('was'), np.str_('were'), np.str_('that'), np.str_('cancer'), np.str_('is'), np.str_('as'), np.str_('by'), np.str_('cells'), np.str_('patients'), np.str_('or')]\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f629d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(512).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73b2e9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 23:46:58.771098: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d04d2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seq in sequences[:5]:\n",
    "#   print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abb1db7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Positive Pairs: 100%|██████████| 46619/46619 [00:00<00:00, 58385.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "targets.shape: (160317,)\n",
      "contexts.shape: (160317, 4)\n",
      "labels.shape: (160317, 4)\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=5,\n",
    "    num_ns=3,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=0)\n",
    "\n",
    "print(f\"\\ntargets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace14268",
   "metadata": {},
   "source": [
    "Gegenereerde data opslaan:\n",
    "Het duurde lang om te genereren dus sla ik het op in een .joblib bestand, dit zal ik ook voor sommige vervolg stappen doen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ae09ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib_list = [targets, contexts, labels]\n",
    "\n",
    "# joblib.dump(joblib_list, \"targets_contexts_labels.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7df2aeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=((TensorSpec(shape=(20,), dtype=tf.int32, name=None), TensorSpec(shape=(20, 4), dtype=tf.int32, name=None)), TensorSpec(shape=(20, 4), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 20\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14e7574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf1c4ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04301a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 8\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e15cd1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e85f6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1768085220.663878   91872 service.cc:152] XLA service 0x7f8bbc002140 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1768085220.663893   91872 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Laptop GPU, Compute Capability 8.6\n",
      "2026-01-10 23:47:00.677880: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1768085220.724569   91872 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  72/8015\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - accuracy: 0.2302 - loss: 1.3866 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1768085220.993679   91872 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8015/8015\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - accuracy: 0.7878 - loss: 0.7305\n",
      "Epoch 2/13\n",
      "\u001b[1m8015/8015\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - accuracy: 0.9032 - loss: 0.2976\n",
      "Epoch 3/13\n",
      "\u001b[1m8015/8015\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.9148 - loss: 0.2371\n",
      "Epoch 4/13\n",
      "\u001b[1m8015/8015\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.9307 - loss: 0.1932\n",
      "Epoch 5/13\n",
      "\u001b[1m8015/8015\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.9503 - loss: 0.1442\n",
      "Epoch 6/13\n",
      "\u001b[1m8015/8015\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.9691 - loss: 0.0949\n",
      "Epoch 7/13\n",
      "\u001b[1m8015/8015\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.9825 - loss: 0.0553\n",
      "Epoch 8/13\n",
      "\u001b[1m8015/8015\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0291\n",
      "Epoch 9/13\n",
      "\u001b[1m8015/8015\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.9957 - loss: 0.0139\n",
      "Epoch 10/13\n",
      "\u001b[1m8015/8015\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0060\n",
      "Epoch 11/13\n",
      "\u001b[1m8015/8015\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0024\n",
      "Epoch 12/13\n",
      "\u001b[1m8015/8015\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 9.2835e-04\n",
      "Epoch 13/13\n",
      "\u001b[1m8015/8015\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 3.7743e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f8ccf554fe0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=13, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46e4fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7df0d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 24999 vectors and metadata entries.\n"
     ]
    }
   ],
   "source": [
    "num_tokens = min(len(vocab), len(weights))\n",
    "\n",
    "with io.open('vectors.tsv', 'w', encoding='utf-8') as out_v, \\\n",
    "     io.open('metadata.tsv', 'w', encoding='utf-8') as out_m:\n",
    "\n",
    "    # Skip de padding token index 0:\n",
    "    for index in range(1, num_tokens):\n",
    "        word = vocab[index]\n",
    "        vec = weights[index]\n",
    "        \n",
    "        clean_word = word.strip()\n",
    "        \n",
    "        if not clean_word:\n",
    "            clean_word = \"[UNK_EMPTY]\"\n",
    "\n",
    "        # Vector\n",
    "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "        \n",
    "        # Metadata\n",
    "        out_m.write(clean_word + \"\\n\")\n",
    "\n",
    "print(f\"{num_tokens - 1} vectors en metadata entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7320b0e5",
   "metadata": {},
   "source": [
    "## Trainen van het model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b8b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingModel.EmbeddingModel()\n",
    "model.load_dataset()\n",
    "model.create_vocab()\n",
    "model.vectorize()\n",
    "model.generate_trainingdata()\n",
    "model.word2vec()\n",
    "model.fit()\n",
    "model.save2file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf7b532",
   "metadata": {},
   "source": [
    "## Data:\n",
    "Als trainingsdata gebruik ik 869597 Pubmed artikelen (~70GB) die ik gedownload heb van https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_bulk/oa_comm/xml/\n",
    "\n",
    "oa_comm_xml.PMC000xxxxxx.baseline.2025-06-26.tar.gz t/m oa_comm_xml.PMC004xxxxxx.baseline.2025-06-26.tar.gz\n",
    "\n",
    "Deze zijn nieuwer dan de artikelen uit 2021 die op /commons/data/NCBI/PubMed/ staan.\n",
    "\n",
    "Vervolgens moeten deze artikelen doorzocht worden op inhoud. \\\n",
    "De .xml bestanden hebben headers voor titel, abstract, body etc. deze doorzoek ik met behulp van mijn klasse: data_parser.py\n",
    "\n",
    "Het filteren op de drie keywords \"cancer\", \"melanoma\" en \"carcinoma\" duurde 9 uur en 30 minuten en reduceerde de hoeveelheid artikelen \n",
    "van 869597 naar 100914 (2.7GB).\n",
    "\n",
    "100914 Artikelen met deze zoekcriteria opgeslagen in: /run/media/floris/FLORIS_3/Data_set/PubMed/dataset_large.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ef1300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = [\"cancer\", \"melanoma\", \"carcinoma\"] # Niet hoofdlettergevoelig...\n",
    "\n",
    "trainings_data = xml_parser(\"/run/media/floris/FLORIS_3/Data_set/PubMed/PMC000xxxxxx/\", \"/home/floris/Downloads/trainingsData3.txt\", keyword_list)\n",
    "\n",
    "#trainings_data.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d70ea50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba0f664b",
   "metadata": {},
   "source": [
    "## Resultaten:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d8b961",
   "metadata": {},
   "source": [
    "Het eerste model was met de volgende hyperparameters getrained:\n",
    "\n",
    "vocabulary grootte: 20000 \\\n",
    "context lengte: 100 \\\n",
    "window_size=5 \\\n",
    "num_ns=4\n",
    "dims = 128\n",
    "\n",
    "Het model was erg overfit op de trainingsdata, de loss was bijna 0 en accuratesse bijna 1. \\\n",
    "De volgende poging zal een minder grote vocabulary krijgen en minder dimenties om beter te generaliseren i.p.v. de trainingsdata te onthouden. \n",
    "\n",
    "De onderstaande afbeelding weergeeft de loss en accuracy grafieken met Tensorboard:\n",
    "\n",
    "![](img/run_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c775c3",
   "metadata": {},
   "source": [
    "Tweede run:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
